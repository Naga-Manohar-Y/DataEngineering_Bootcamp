# User Journey with Perplexity AI

## Discovery
I discovered Perplexity AI as a platform offering AI-powered search and conversational capabilities. Its promise of concise and accurate information immediately caught my attention.

## Onboarding
My initial interaction with Perplexity AI was smooth and intuitive. The clean UI and simple search bar encouraged me to start typing questions immediately. The real-time results with source citations gave me confidence in its reliability.

## Active Use
Over time, I found myself using Perplexity AI for research, brainstorming ideas, and answering complex questions. Features like follow-up questions, contextual understanding, and multi-step reasoning stood out, significantly improving my efficiency compared to traditional search engines.

## Current Use
Today, I use Perplexity AI as a trusted assistant for quick, authoritative answers. I also leverage it for exploring new topics, thanks to its ability to synthesize information from multiple sources.

---

# Three Experiments to Improve the User Experience

## 1. Personalization Experiment
- **Objective**: Test if personalized recommendations based on user search history improve engagement.
- **Test Cells**:
  - **Control**: No personalization (current experience).
  - **Test Group A**: Show tailored topic suggestions based on past searches.
  - **Test Group B**: Highlight trending questions and topics in the userâ€™s area of interest.
- **Hypothesis**: Personalization will increase engagement metrics like average session duration and repeat visits.
- **Leading Metrics**: Click-through rate (CTR) on recommendations, time spent on the platform.
- **Lagging Metrics**: Retention rate, user satisfaction (measured via surveys).

---

## 2. Interactive Explanation Experiment
- **Objective**: Assess if interactive, step-by-step breakdowns of complex topics enhance user comprehension.
- **Test Cells**:
  - **Control**: Standard text-based explanations.
  - **Test Group A**: Include expandable sections for detailed explanations.
  - **Test Group B**: Add interactive diagrams or flowcharts for visualization.
- **Hypothesis**: Interactive content will improve comprehension and trust in the platform.
- **Leading Metrics**: Interaction rate (clicks on expandable sections or diagrams).
- **Lagging Metrics**: User satisfaction scores, increase in users returning for educational queries.

---

## 3. Collaboration Feature Experiment
- **Objective**: Determine if collaborative tools (e.g., shared queries or workspaces) enhance platform adoption among teams or study groups.
- **Test Cells**:
  - **Control**: No collaboration features (current experience).
  - **Test Group A**: Enable sharing of individual queries with notes.
  - **Test Group B**: Introduce collaborative workspaces for shared research.
- **Hypothesis**: Collaboration features will boost adoption by teams and drive platform growth.
- **Leading Metrics**: Number of shared queries, usage of workspaces.
- **Lagging Metrics**: New user registrations from shared invites, team-based retention rates.

---

# Summary of Hypotheses and Metrics

| **Experiment**              | **Hypothesis**                                      | **Leading Metrics**                      | **Lagging Metrics**                      |
|-----------------------------|---------------------------------------------------|-----------------------------------------|-----------------------------------------|
| **Personalization**          | Increases engagement and satisfaction.            | CTR on recommendations, session time.   | Retention rate, user satisfaction.      |
| **Interactive Explanation** | Improves comprehension and trust.                 | Interaction rate on new features.       | Survey-based satisfaction, return rate. |
| **Collaboration Feature**   | Drives team adoption and platform growth.         | Usage of collaboration tools.           | New registrations, team retention.      |
